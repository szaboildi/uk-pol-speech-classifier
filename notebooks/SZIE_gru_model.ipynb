{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b7190d-ca26-4c63-a83f-851422ce2488",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44ba32-5c58-4b99-983b-3ec26af16dac",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93485f85-5771-4c8a-8be3-1187b53ca99d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.3.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7924e3e-771a-4166-ba92-04901eff485c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1446c46c-f4bb-4011-8ab7-9d849bd74a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:29:12.869717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-10 09:29:14.146667: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-05-10 09:29:14.146805: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-05-10 09:29:14.146816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Masking, GRU, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9139ebe0-d72a-4528-b210-6f4a921fc63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../processed_data/features_1000sample_400min_600cutoff_for_embed.csv\")\n",
    "y = pd.read_csv(\"../processed_data/target_1000sample_400min_600cutoff_for_embed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fcd2d6-6b37-48a9-ac2b-d6e6f61bb10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codes = {p: i for i, p in enumerate(y[\"party\"].unique())}\n",
    "# y = y[\"party\"].map(codes)\n",
    "y = OneHotEncoder(sparse_output=False).fit_transform(y[\"party\"].values.reshape(-1, 1))\n",
    "X = X[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad267388-22a4-493e-b939-7892d6cd3eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000,), (7000, 7))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cf940-35c4-4b2e-a938-1f9e1a22dfd5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80120b28-4fe3-466c-9de6-57eb11c77525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865f4d5-da88-430d-b487-b011a71231e3",
   "metadata": {},
   "source": [
    "### Embed the training and test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5529d597-b059-41b9-82e9-27fc625ad27a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec and word not in stop_words:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence.split())\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfcbfff7-7a89-4891-9f34-09a995fa96e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec_model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248f68ca-285f-4891-bcd1-c00f6ae8b896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_embed = embedding(word2vec_model, X_train)\n",
    "X_test_embed = embedding(word2vec_model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e2f88-9491-4643-9c15-74791472bc7c",
   "metadata": {},
   "source": [
    "### Pad sequences to ensure uniform input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980a9a10-ffc0-4022-9165-83878ff4ea1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max(max([len(x) for x in X_train_embed]), max([len(x) for x in X_test_embed])) # Maximum sequence length\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b33c4ab9-e5dc-4668-9287-9a0c81507df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f953a36-7196-4228-befa-f433f61fd1f4",
   "metadata": {},
   "source": [
    "### Tokenize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e14ba8-0d87-4b43-95ae-01c60f25fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=10000)  # Limit vocabulary size\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3dbfc-a723-4e60-bf61-564f2b46fbe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b275137-b08c-4930-a237-cb32e6db3084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Convert words to Word2Vec embeddings\n",
    "# word_index = tokenizer.word_index\n",
    "# embedding_matrix = np.zeros((len(word_index) + 1, 300))  # Assuming Word2Vec vectors are 300-dimensional\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if word in word2vec_model:\n",
    "#         embedding_matrix[i] = word2vec_model[word]\n",
    "\n",
    "# # Define the model\n",
    "# embedding_layer = tf.keras.layers.Embedding(len(word_index) + 1,\n",
    "#                                             300,  # Assuming Word2Vec vectors are 300-dimensional\n",
    "#                                             weights=[embedding_matrix],\n",
    "#                                             input_length=maxlen,\n",
    "#                                             trainable=False)  # Freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22680562-6fc4-4925-9843-e78226998b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Masking())\n",
    "model.add(GRU(128, activation=\"tanh\", dropout=0.4, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(GRU(128, activation=\"tanh\", dropout=0.4, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dddba6-c7c7-4c0d-a2aa-febc6ed41a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 71/140 [==============>...............] - ETA: 39s - loss: 1.9638 - accuracy: 0.1426"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(np.array(X_train_pad), np.array(y_train), epochs=100, batch_size=32, \n",
    "          validation_split=0.2, callbacks=[es], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d0efe-d3fe-4da3-8f8b-8849ece830ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15d89e-06b8-458e-b470-ffc5442b3f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
