{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b7190d-ca26-4c63-a83f-851422ce2488",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44ba32-5c58-4b99-983b-3ec26af16dac",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93485f85-5771-4c8a-8be3-1187b53ca99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1446c46c-f4bb-4011-8ab7-9d849bd74a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Masking, GRU, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9139ebe0-d72a-4528-b210-6f4a921fc63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../processed_data/features_1000sample_400min_600cutoff.csv\")\n",
    "y = pd.read_csv(\"../processed_data/target_1000sample_400min_600cutoff.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5fcd2d6-6b37-48a9-ac2b-d6e6f61bb10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codes = {p: i for i, p in enumerate(y[\"party\"].unique())}\n",
    "# y = y[\"party\"].map(codes)\n",
    "y = OneHotEncoder(sparse_output=False).fit_transform(y[\"party\"].values.reshape(-1, 1))\n",
    "X = X[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad267388-22a4-493e-b939-7892d6cd3eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000,), (7000, 7))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cf940-35c4-4b2e-a938-1f9e1a22dfd5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80120b28-4fe3-466c-9de6-57eb11c77525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865f4d5-da88-430d-b487-b011a71231e3",
   "metadata": {},
   "source": [
    "### Embed the training and test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5529d597-b059-41b9-82e9-27fc625ad27a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word not in stop_words and word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence.split())\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfcbfff7-7a89-4891-9f34-09a995fa96e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "248f68ca-285f-4891-bcd1-c00f6ae8b896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_embed = embedding(word2vec_model, X_train)\n",
    "X_test_embed = embedding(word2vec_model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e2f88-9491-4643-9c15-74791472bc7c",
   "metadata": {},
   "source": [
    "### Pad sequences to ensure uniform input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5403e5b4-01b0-4d58-823c-ae1a46f197bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, [244, 273, 299, 285, 238, 290, 303, 288, 278, 259])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max(max([len(x) for x in X_train_embed]), max([len(x) for x in X_test_embed]))  # Maximum sequence length\n",
    "maxlen, [len(x) for x in X_train_embed[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "980a9a10-ffc0-4022-9165-83878ff4ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f953a36-7196-4228-befa-f433f61fd1f4",
   "metadata": {},
   "source": [
    "### Tokenize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e14ba8-0d87-4b43-95ae-01c60f25fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=10000)  # Limit vocabulary size\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3dbfc-a723-4e60-bf61-564f2b46fbe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b275137-b08c-4930-a237-cb32e6db3084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Convert words to Word2Vec embeddings\n",
    "# word_index = tokenizer.word_index\n",
    "# embedding_matrix = np.zeros((len(word_index) + 1, 300))  # Assuming Word2Vec vectors are 300-dimensional\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if word in word2vec_model:\n",
    "#         embedding_matrix[i] = word2vec_model[word]\n",
    "\n",
    "# # Define the model\n",
    "# embedding_layer = tf.keras.layers.Embedding(len(word_index) + 1,\n",
    "#                                             300,  # Assuming Word2Vec vectors are 300-dimensional\n",
    "#                                             weights=[embedding_matrix],\n",
    "#                                             input_length=maxlen,\n",
    "#                                             trainable=False)  # Freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22680562-6fc4-4925-9843-e78226998b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Masking())\n",
    "model.add(GRU(150, activation=\"tanh\", return_sequences=True, recurrent_dropout=0.4, dropout=0.2,\n",
    "             kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(GRU(150, recurrent_dropout=0.4, dropout=0.2, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(75, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "es = EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f36f46-54fc-42cb-8dba-ede2ef6f70bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "140/140 [==============================] - 157s 1s/step - loss: 1.9388 - accuracy: 0.1710 - val_loss: 1.8977 - val_accuracy: 0.2000\n",
      "Epoch 2/100\n",
      "140/140 [==============================] - 156s 1s/step - loss: 1.8913 - accuracy: 0.2085 - val_loss: 1.8596 - val_accuracy: 0.2214\n",
      "Epoch 3/100\n",
      "140/140 [==============================] - 151s 1s/step - loss: 1.7523 - accuracy: 0.2692 - val_loss: 1.5835 - val_accuracy: 0.3205\n",
      "Epoch 4/100\n",
      "140/140 [==============================] - 152s 1s/step - loss: 1.5937 - accuracy: 0.3270 - val_loss: 1.5467 - val_accuracy: 0.3134\n",
      "Epoch 5/100\n",
      "140/140 [==============================] - 151s 1s/step - loss: 1.4702 - accuracy: 0.3737 - val_loss: 1.3686 - val_accuracy: 0.4384\n",
      "Epoch 6/100\n",
      "140/140 [==============================] - 152s 1s/step - loss: 1.3395 - accuracy: 0.4478 - val_loss: 1.3765 - val_accuracy: 0.4304\n",
      "Epoch 7/100\n",
      "140/140 [==============================] - 151s 1s/step - loss: 1.2865 - accuracy: 0.4725 - val_loss: 1.2499 - val_accuracy: 0.4705\n",
      "Epoch 8/100\n",
      "140/140 [==============================] - 151s 1s/step - loss: 1.2449 - accuracy: 0.4904 - val_loss: 1.2385 - val_accuracy: 0.4955\n",
      "Epoch 9/100\n",
      "140/140 [==============================] - 151s 1s/step - loss: 1.2184 - accuracy: 0.5054 - val_loss: 1.2286 - val_accuracy: 0.4875\n",
      "Epoch 10/100\n",
      "140/140 [==============================] - 152s 1s/step - loss: 1.1845 - accuracy: 0.5219 - val_loss: 1.2021 - val_accuracy: 0.5054\n",
      "Epoch 11/100\n",
      "140/140 [==============================] - 151s 1s/step - loss: 1.1674 - accuracy: 0.5252 - val_loss: 1.2194 - val_accuracy: 0.4991\n",
      "Epoch 12/100\n",
      "140/140 [==============================] - 153s 1s/step - loss: 1.1332 - accuracy: 0.5350 - val_loss: 1.1894 - val_accuracy: 0.4973\n",
      "Epoch 13/100\n",
      "140/140 [==============================] - 151s 1s/step - loss: 1.1025 - accuracy: 0.5605 - val_loss: 1.1940 - val_accuracy: 0.5196\n",
      "Epoch 14/100\n",
      "140/140 [==============================] - 154s 1s/step - loss: 1.0837 - accuracy: 0.5621 - val_loss: 1.1776 - val_accuracy: 0.5312\n",
      "Epoch 15/100\n",
      "140/140 [==============================] - 152s 1s/step - loss: 1.0388 - accuracy: 0.5868 - val_loss: 1.1704 - val_accuracy: 0.5437\n",
      "Epoch 16/100\n",
      "140/140 [==============================] - 155s 1s/step - loss: 1.0101 - accuracy: 0.5938 - val_loss: 1.1586 - val_accuracy: 0.5500\n",
      "Epoch 17/100\n",
      "140/140 [==============================] - 152s 1s/step - loss: 0.9798 - accuracy: 0.6054 - val_loss: 1.1843 - val_accuracy: 0.5295\n",
      "Epoch 18/100\n",
      " 31/140 [=====>........................] - ETA: 1:50 - loss: 0.9360 - accuracy: 0.6300"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(np.array(X_train_pad), np.array(y_train), epochs=100, batch_size=32, \n",
    "          validation_split=0.2, callbacks=[es], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d0efe-d3fe-4da3-8f8b-8849ece830ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15d89e-06b8-458e-b470-ffc5442b3f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
