{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed00559-7ea9-4cb3-a1cb-f819694a7d4c",
   "metadata": {},
   "source": [
    "# Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34615d-aa44-412a-baaa-7d450a061fef",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b602ee-fac0-4cf2-b2ea-71e19af9dbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b774bc-bba2-4bab-85c2-54c92af52071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2be04-c4c5-47d8-a47d-557ba2819226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import Masking, Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from polclassifier.params import *\n",
    "\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136bde5-19ab-4849-b6cd-ec81fb0f832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"~/code/szaboildi/uk-pol-speech-classifier/processed_data/features_1000sample_400min_600cutoff_for_embed.csv\")\n",
    "y = pd.read_csv(\"~/code/szaboildi/uk-pol-speech-classifier/processed_data/target_1000sample_400min_600cutoff_for_embed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec297fd5-be60-4334-81a9-fe64f291015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c43c81-4844-421d-96c5-aff12a0a8456",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7d372-64b5-4ba2-8a26-8f0b9b99beba",
   "metadata": {},
   "source": [
    "### Embed the training and test sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12dc4ec-4c4d-4680-a3b8-37c57a539b46",
   "metadata": {},
   "source": [
    "feed raw data, you need text-classification tfautomodel sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6120e00-2fe4-46ba-a4e5-a2e3ea41ee5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd544e5-7870-4454-b220-c1b1a5394d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL, padding_side = \"right\")\n",
    "\n",
    "tokenized_tensors = tokenizer(X[\"text\"].tolist(), \n",
    "                              max_length=512, \n",
    "                              padding = \"max_length\", \n",
    "                              truncation = True, \n",
    "                              return_tensors=\"tf\")\n",
    "\n",
    "model = TFAutoModel.from_pretrained(HF_MODEL, \n",
    "                                    from_pt = True, \n",
    "                                    num_labels=7)\n",
    "\n",
    "embeddings = model.predict(tokenized_tensors[\"input_ids\"])\n",
    "\n",
    "X_embed = embeddings.last_hidden_state[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ace3a-1550-4a99-a99f-67c18da24088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embed, y)\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "input_shape = (128,) \n",
    "\n",
    "dense_model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),  # Add dropout for regularization\n",
    "    Dense(175, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),  # Add dropout for regularization\n",
    "    Dense(120, activation='relu'),\n",
    "    Dense(7, activation='softmax')])\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return 0.001 * np.exp(-epoch / 10)\n",
    "\n",
    "dense_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = dense_model.fit(X_train, \n",
    "                          y_train, \n",
    "                          validation_split=0.2, \n",
    "                          epochs=100,\n",
    "                          batch_size=32,\n",
    "                          callbacks=[es, LearningRateScheduler(lr_schedule)]\n",
    "                         )\n",
    "\n",
    "dense_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fa93a-b5f5-4886-b96e-916250a6abe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Learning Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curve(history)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
